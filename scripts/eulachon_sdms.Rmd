---
title: "Eulachon eDNA SDMs"
author: "Owen R. Liu"
date: "2024-02-09"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
options(dplyr.summarise.inform=FALSE)
library(tidyverse)
library(here)
library(tictoc)
library(sf)
library(viridis)
library(ggsci)
library(cowplot)
library(rnaturalearth)
library(marmap)
library(RANN)
library(contoureR)
library(sdmTMB)
library(nngeo)
library(contoureR)
library(corrplot)
```

# Purpose

Build species distribution models in `sdmTMB` for eulachon, based on spatiotemporal data on eDNA for eulachon (*Thaleichthys pacificus*). The data are structured in three dimensions (latitude, longitude, depth), or four including year, and so we want to build a flexible set of models that can take advantage of this dimensionality of the data to create high-resolution predictions.

We will use environmental covariates to constrain our predictions, based on temperature, salinity, and bottom depth, as well as biological productivity variables. In spatiotemporal models, though, we also need to decide how to guide the model to partition variation across different spatial and depth fields (i.e., the spatially autocorrelated parts of the model that are estimated in parallel to the fitted covariates). To explore these options, we decided to try models with different combinations of spatial fields and intercepts:

Spatial field options to try: 

*   One common spatial field across all data
*   spatial field by depth
*   spatial field by year
*   spatial field by depth and year

Intercepts options to try

*   single intercept
*   random intercept by depth category
*   random intercept by year
*   random intercept by depth and year

Combinations of spatial fields and intercepts gives us 16 models to consider. Some of these models will certainly break and/or not converge, particularly the more complex versions.

# Pre-processing

Import eDNA and covariate data, designate offsets, and build prediction grid. Note that cleaning and joining of the standards and unknown samples has been done in other scripts---one to process the 2019 data, one for 2021, and one to join them. If desired, you can re-run those here, but they are not evaluated out for now to save time.

```{r,eval=F,warning=F,message=F}
rmarkdown::render(here::here('scripts','process 2019 eulachon data.Rmd'),quiet=TRUE)
rmarkdown::render(here::here('scripts','process 2021 eulachon data.Rmd'),quiet=TRUE)
source(here::here('scripts','join_eulachon_qPCR_data.R'))

rm(list=ls())
```


## Import Data

Spatial coordinate reference system we will use for thesse analyses:

```{r}
pred.crs <- terra::rast(here('data','raster_grid_blake','fivekm_grid.tif')) %>% st_crs()
```


Load in our cleaned and joined eDNA data.

```{r}
# qPCR standards
d <- read_rds(here('data','qPCR',"eulachon qPCR 2019 and 2021 standards clean.rds")) %>% 
  mutate(Ct=replace_na(Ct,0))# delta-models in sdmTMB need this

# field samples (already joined to physical GLORYS data)
d_obs <- read_rds(here('data','eDNA_glorys_matched.rds'))
d_obs_bgc <- read_rds(here('data','eDNA_glorys_bgc_matched.rds')) %>% 
  dplyr::select(chl:phyc)
d_obs <- d_obs %>% 
  bind_cols(d_obs_bgc) %>% 
  mutate(Ct=replace_na(Ct,0)) %>% # delta-models in sdmTMB need this
  mutate(utm.lon.km=utm.lon.m/1000,
         utm.lat.km=utm.lat.m/1000)

# positive samples by depth/year
d_obs %>% 
  filter(Ct>0) %>% 
  count(year,depth_cat) %>% 
  ggplot(aes(depth_cat,n,fill=factor(year)))+
  labs(x="Depth Category",y="Number of Positive Observations",fill="Year")
  geom_col(position='dodge')

# field samples, filtered such that we are only including depth categories 0 (surface), 50m, and 150m
d_obs_filt <- d_obs %>% filter(depth_cat %in% c(0,50,150))

d_obs_sf <- d_obs_filt %>% st_as_sf(coords=c('utm.lon.m','utm.lat.m'),crs=pred.crs)
```

## Covariate data

Bathymetry/bottom depth was already attached to the samples during pre-processing. The field samples were also joined to modeled GLORYS data from their [Global Ocean Physics Reanalysis](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description). See script `match_GLORYS_temp_salinity.R`.

### Krill/Euphausiid data

These data on the relative abundance of krill come from Beth Phillips, and are organized in the script `krill_nasc_matching`. As with the raw data cleaning above, we do not evaluate this chunk here and just use the output; but the user can run it if they wish. Be warned that it will take awhile to run.

```{r,eval=F}
rmarkdown::render(here::here('scripts','krill_nasc_matching.Rmd'),quiet=TRUE)
```

```{r}
# Load krill data
edna_krill_metrics <- read_rds(here('model output','edna_krill_metrics.rds')) %>% 
  filter(depth_cat %in% c(0,50,150))
  
d_obs_filt <- d_obs_filt %>% bind_cols(edna_krill_metrics %>% dplyr::select(k1:k7))
```

### River Discharge data

We also have a dataset of mean river discharge for US West Coast rivers, which we have projected onto our ocean model grid using exponential distance weighting (see script `river_discharge`). Basically, this allows us to explore a variable representing the distance of each spatial grid cell to river mouths, weighted by distance between river and grid cell, as well as by the magnitude of annual discharge from the river.

```{r}
riverine_influence <- read_rds(here('data','river_influence_grid_matched.rds'))
```

## Center Covariates

```{r}
#center and scale covariates
# d_obs_filt <- d_obs_filt %>% 
#   # make bottom depth positive
#   mutate(bathy.bottom.depth=-bathy.bottom.depth) %>% 
#   # center and scale covariates
#   mutate(across(c(so,thetao,bathy.bottom.depth,k1,k2,k3,k4,k5),list(norm=function(x) as.numeric(scale(x)))))
```

```{r}
# normalized_vars_p <- d_obs_filt %>% 
#   select(contains("norm")) %>% 
#   pivot_longer(everything(),names_to="variable",values_to="val") %>% 
#   ggplot(aes(val,fill=variable))+
#   geom_density()+
#   facet_wrap(~variable,scales='free')
# normalized_vars_p

```

## Log Covariates

Try natural logging covariates to scale them better for fitting

```{r}
#log covariates
d_obs_filt <- d_obs_filt %>% 
  # make bottom depth positive
  mutate(bathy.bottom.depth=-bathy.bottom.depth) %>% 
  # log covariates
  mutate(across(c(so,thetao,bathy.bottom.depth,k1,k2,k3,k4,k5,k6,k7,chl,no3,nppv,o2,phyc),list(ln=function(x){
    x[x==0]<-1
    log(x)
    })))
```

```{r}
logged_vars_p <- d_obs_filt %>% 
  select(contains("_ln")) %>% 
  pivot_longer(everything(),names_to="variable",values_to="val") %>% 
  ggplot(aes(val,fill=variable))+
  geom_density()+
  facet_wrap(~variable,scales='free')
logged_vars_p
```

## Other Transformations

Here we add some other transformations of the above variables in case we want to try them in fitting

```{r}
d_obs_filt <- d_obs_filt %>% 
  # squares
  mutate(depth_ln2=bathy.bottom.depth_ln^2) %>% 
  mutate(thetao_ln2=thetao_ln^2,
         so_ln2=so_ln^2)

# defining factors for depth category and year
d_obs_filt$depth_cat <- factor(d_obs_filt$depth_cat)

d_obs_filt$yr_fct <- as.factor(d_obs_filt$year)
```



## Designate Offsets

While we are primarily interested in eulachon DNA concentration over space, time, and depth, we do not directly measure this concentration with our observations. Rather, we have replicate qPCR observations assocated with a water sample taken from a Niskin bottle and we have to account for the various modifications that have occurred during water sampling and processing. As part of this, we have a series of offsets that modify the true DNA concentration to affect what we observed in the qPCR. These [offsets](https://pbs-assess.github.io/sdmTMB/articles/model-description.html#offset-terms) will go into the model as log-transformed variables (because we will use a log link), and are scaling factors without estimated coefficients.

We are primarily concerned with three offsets: one for the volume filtered out of each 2.5L Niskin bottle for each sample (`ln_vol_offset`). Second, some samples were inhibited in PCR, and were diluted to eliminate this inhibition (`ln_dil_offset`). Third, there was a wash error with some samples in 2019 that we correct for with a fixed effect.

Finally, we include an expansion factor of 20 to convert our estimates to the standard measure of eDNA copies/L. We derived this assuming a 2.5L water sample eluted in 100uL Longmire and 2 uL used in each PCR reaction. Therfore, each 2uL of sample corresponds to 2% (2uL of 100uL) of the total sample and therefore is equivalent to the copies DNA contained in 50mL of water. (0.02 x 2500 ml = 50 ml). So, multiplying by 20 gets us to copies/L.

## INLA Mesh

Create the mesh that will be the basis of our estimation---based on the spatial distribution of our sample data, the mesh is created in order to facilitate efficient processing and estimatino of the model in sdmTMB. Instead of calculating enormous covariance matrices in model fitting, we can drastically reduce computation time by using the mesh.

NOTE: SEE THE BOTTOM OF THIS SCRIPT FOR SOME DIAGNOSTICS ON MESH COMPLEXITY

```{r}
locs <- d_obs_filt %>%
  distinct(year,station,utm.lon.km,utm.lat.km) %>%
  st_as_sf(coords=c("utm.lon.km", "utm.lat.km"))

max.edge = diff(range(st_coordinates(locs)[,1]))/4
bound.outer = diff(range(st_coordinates(locs)[,1]))/3

domain <-fmesher::fm_nonconvex_hull(locs,
                              concave = -0.025,
                              convex = -0.025)
# I think we can use this to subset our prediction grid

inla_mesh <- fmesher::fm_mesh_2d_inla(
  #loc=locs[,c("utm.lon","utm.lat")],
  loc.domain = domain, # coordinates
  boundary=domain,
  max.edge = c(max.edge,max.edge*10), # max triangle edge length; inner and outer meshes
  offset = c(max.edge, bound.outer),  # inner and outer border widths
  #max.n.strict=100,#,
  min.angle=15,
  cutoff = max.edge/4 # minimum triangle edge length
)

mesh <- make_mesh(d_obs_filt,c("utm.lon.km", "utm.lat.km"),mesh=inla_mesh) # this was decided based on a cross-validation exercise (see "Test Mesh Complexity" below)
```

```{r}
# as of 2/13/24, these were the mesh options setup that Ole is using for hake
# locs <- d_obs_filt %>%
#   distinct(year,station,utm.lon.km,utm.lat.km) %>% 
#   st_as_sf(coords=c("utm.lon.km", "utm.lat.km"))
# domain <-fmesher::fm_nonconvex_hull(locs,
#                               concave = -0.025,
#                               convex = -0.025)
# 
# inla_mesh <- fmesher::fm_mesh_2d_inla(
#   #loc=locs[,c("utm.lon","utm.lat")],
#   loc.domain = domain, # coordinates
#   boundary=domain,
#   max.edge = c(40, 1000), # max triangle edge length; inner and outer meshes
#   offset = c(30, 80),  # inner and outer border widths
#   #max.n.strict=100,#,
#   cutoff = 64 , # minimum triangle edge length
#   min.angle=20
# )


# 
# mesh <- make_mesh(d_obs_filt, c("utm.lon.km", "utm.lat.km"), mesh = inla_mesh)
# mesh2 <- make_mesh(d_obs_filt, c("utm.lon.km", "utm.lat.km"), cutoff=20)
# mesh$mesh$n
# mesh2$mesh$n
# plot(mesh)
# plot(mesh2)
```

Now we are ready to start fitting models. We do this in two stages: first, we compare model structures to pick an appropriate structural format for sdmTMB given our data. Then, we use variations on our chosen model to choose appropriate environmental covariates to choose.

# Fit Models

In the first stage of model selection, we decide on an appropriate model structure, referring to our choices about specifications of intercepts and type of latent spatial field. All models in this initial stage will include a spatial field of some flavor, as well as penalized splines for temperature and depth covariates. They will vary structurally in the specification of the spatial field(s) and the types of offsets/intercepts we impose.

Spatial field options to try: 

*   One common spatial field across all data
*   spatial field by depth
*   spatial field by year
*   spatial field by depth and year

Intercepts options to try

*   single intercept
*   random intercept by depth category
*   random intercept by year
*   random intercept by depth and year

We will do this one at a time, increasing slowly in complexity. In the second stage, we try different environmental covariates

```{r}
# source the plotting utilities
source(here("scripts","plotting_utils.R"))
```

As a measure of model fit, we use 3-fold cross-validation with replacement. Using the same folds across all candidate models, we can pull the summed log likelihood from cross-validation as a measure of model skill.

```{r}
set.seed(8913)
clust <-sample(seq_len(3), size = nrow(d_obs_filt), replace = TRUE)
```


## Common Spatial Field

```{r}
tic("Fitting:")
f0 <- sdmTMB(
  Ct ~ 1+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f0
paste("AIC:",AIC(f0))
make_pred_obs_plots(f0,d,model_name="Fit0")

tic("Calculating 3-fold cross-validation:")
f0_cv <- sdmTMB_cv(
  Ct ~ 1+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f0_cv$sum_loglik)
```

First, a model with just one common spatial field, and a single intercept.

```{r}
tic("Fitting:")
f1 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f1
paste("AIC:",AIC(f1))
make_pred_obs_plots(f1,d,model_name="Fit1")

tic("Calculating 3-fold cross-validation:")
f1_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f1_cv$sum_loglik)

make_cond_plot(f1,bathy.bottom.depth_ln,exp_var=T,saveplot=F)
```

One spatial field, random intercept by depth category. Salinity doesn't seem to work as a smooth, use a linear function instead.

```{r}
tic("Fitting:")
f2 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f2
paste("AIC:",AIC(f2))
make_pred_obs_plots(f2,d,model_name="Fit2")

tic("Calculating 3-fold cross-validation:")
f2_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f2_cv$sum_loglik)

make_cond_plot(f2,bathy.bottom.depth_ln,exp_var=T,saveplot=F)

```

One spatial field, random intercept by year.

```{r}
tic("Fitting:")
f3 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f3
paste("AIC:",AIC(f3))
make_pred_obs_plots(f3,d,model_name="Fit3")

tic("Calculating 3-fold cross-validation:")
f3_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f3_cv$sum_loglik)

make_cond_plot(f3,bathy.bottom.depth_ln,exp_var=T,saveplot=F)
```

```{r}
# make_pred_obs_plots(f3,d,model_name="Fit3")
# make_cond_plot(f3,thetao_ln,exp_var=T,saveplot=F)+labs(x="Temperature (C)",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
# make_cond_plot(f3,bathy.bottom.depth_ln,exp_var=T,saveplot=F)+labs(x="Bottom Depth (m)",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
# make_cond_plot(f3,so_ln,exp_var=T,saveplot=F)+labs(x="Salinity (psu)")
# make_cond_plot(f3,k7_ln,exp_var=T,saveplot=F)+labs(x="Krill KDE",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
# make_cond_plot(f3,nppv_ln,exp_var=T,saveplot=F)+labs(x="NPPV",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
```


```{r,fig.height=8,fig.width=6}
f3_abun_lat <- make_lat_abundance(f3)+theme(text=element_text(size=14))
ggsave(here('plots','f3_abun_by_lat.png'),f3_abun_lat,h=8,w=6,bg='white')
```

One spatial field, random intercept by year and depth category

```{r}
tic("Fitting:")
f4 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f4
paste("AIC:",AIC(f4))
make_pred_obs_plots(f4,d,model_name="Fit4")

tic("Calculating 3-fold cross-validation:")
f4_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f4_cv$sum_loglik)
```

## Spatial Field by Depth

Spatial field by depth category, single intercept.

```{r}
# make dummy factors for depth categories
m <- model.matrix(Ct ~ depth_cat, data = d_obs_filt)
d_obs_filt$d1 <- m[,1]
d_obs_filt$d2 <- m[,2]
d_obs_filt$d3 <- m[,3]
```
 
```{r}
tic("Fitting:")
f5 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  spatial_varying= ~d1+d2+d3,
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f5
paste("AIC:",AIC(f5))
make_pred_obs_plots(f5,d,model_name="Fit5")

tic("Calculating 3-fold cross-validation:")
f5_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatial_varying= ~d1+d2+d3,
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f5_cv$sum_loglik)
# had difficulty partitioning between sigma_o (spatial field) and sigma_z (spatially varying coefficients)

```

Spatial field by depth category, random intercept by depth category. This one feels weird

```{r}
tic("Fitting model: ")
f6 <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial_varying= ~d1+d2+d3,
  spatiotemporal="off",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f6
# probably too many terms (see sanity(f))
sanity(f6)
make_pred_obs_plots(f6,d,model_name="Fit6")
```

Spatial field by depth category, random intercept by year.

```{r}
tic("Fitting model: ")
f7 <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial_varying= ~d1+d2+d3,
  spatiotemporal="off",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f7
make_pred_obs_plots(f7,d,model_name="Fit7")
```

A version of the above turning the basic spatial field off (because of the depth fields), and removing bottom depth as a predictor

```{r}
tic("Fitting model: ")
f7.2 <- sdmTMB(
  Ct ~ 0+s(thetao_ln,k=3)+so_ln+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial_varying= ~d1+d2+d3,
  spatiotemporal="off",
  spatial="off",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f7.2
make_pred_obs_plots(f7.2,d,model_name="Fit7.2")
# this one ran fine
```

**MODELS BELOW HERE DON'T WORK WELL RIGHT NOW*

Spatial field by depth category, random intercept by depth and year

```{r}
tic("Fitting model: ")
f8 <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(k2_ln,k=3)+poly(thetao_ln,2)+poly(so_ln,2)+washed+(1|depth_cat)+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial_varying= ~d1+d2+d3,
  spatiotemporal="off",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f8
# DID NOT RUN
# make_pred_obs_plots(f8,d,model_name="Fit8")
```

## Spatial Field by Year

Spatial field by year, single intercept.

```{r}
tic("Fitting model: ")
f9 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(k2_ln,k=3)+poly(thetao_ln,2)+poly(so_ln,2)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f9
# DID NOT APPROPRIATELY CONVERGE- HAVING TROUBLE WITH SPATIAL SD
make_pred_obs_plots(f9,d,model_name="Fit9")
```

Spatial field by year, random intercept by depth category.

```{r}
tic("Fitting model: ")
f10 <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(k2_ln,k=3)+poly(thetao_ln,2)+poly(so_ln,2)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f10
#AIC 
# worked a little better, but spatial SD very small
make_pred_obs_plots(f10,d,model_name="Fit10")
```

Spatial field by year, random intercept by year.

```{r}
tic("Fitting model: ")
f11 <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(k2_ln,k=3)+poly(thetao_ln,2)+poly(so_ln,2)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f11 # similar issues as above
#AIC 
```

Spatial field by year, random intercept by depth and year

```{r}
tic("Fitting model: ")
f12 <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(k2_ln,k=3)+poly(thetao_ln,2)+poly(so_ln,2)+washed+(1|depth_cat)+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f12 # same issues as above
```

Where we're at so far: models 1 through 4 converged with no issues, and relatively quickly. Models 5-8 seemingly had difficulty apportioning variance between the basic spatial field and the spatially varying coefficients for depth. They converged, but with significant warnings (see,e.g., `sanity(f8)`). Models 9-12 did not appropriately converge.

# Conditional Effects

We can look at the estimated effects of bottom depth, temperature, salinity and krill on our estimates of eulachon eDNA, conditional on all other variable being held at their means. For now, we use a range of models that converged, and whose diagnostic plots above (e.g. standard curve fits) look reasonable.

## Bottom Depth

```{r}
depth.cond <- purrr::map2_df(list(f2,f3,f4,f5,f7.2),c("f2","f3","f4","f5","f7.2"),function(m,n){
  pr <- make_cond_plot(m,bathy.bottom.depth_ln,exp_var = T,return_dat=T,saveplot = F)
  pr %>% mutate(model=n)
})

depth.cond.p <- depth.cond %>%   
  ggplot(aes(bathy.bottom.depth_ln, exp(est),ymin=ymin,ymax=ymax,
  fill=model
)) +
  geom_line() +
  geom_ribbon(alpha = 0.4) +
  scale_x_continuous(limits=c(0,800)) +
  coord_cartesian(expand = F) +
  facet_wrap(~model)+
  labs(x = "Bottom Depth (m)", y = "eDNA conc.",title="Conditional Effect of Bottom Depth")
depth.cond.p
```

## Temperature

```{r}
thetao.cond <- purrr::map2_df(list(f2,f3,f4,f5,f7.2),c("f2","f3","f4","f5","f7.2"),function(m,n){
  pr <- make_cond_plot(m,thetao_ln,exp_var = F,return_dat=T,saveplot = F)
  pr %>% mutate(model=n)
})

thetao.cond.p <- thetao.cond %>%   
  ggplot(aes(thetao_ln, exp(est),ymin=ymin,ymax=ymax,
  fill=model
)) +
  geom_line() +
  geom_ribbon(alpha = 0.4) +
  coord_cartesian(expand = F) +
  facet_wrap(~model)+
  labs(x = "Temperature (C)", y = "eDNA conc.",title="Conditional Effect of Temperature")
thetao.cond.p
```

## Salinity

This one isn't that exciting/informative: consider removing salinity?

```{r}
so.cond <- purrr::map2_df(list(f2,f3,f4,f5,f7.2),c("f2","f3","f4","f5","f7.2"),function(m,n){
  pr <- make_cond_plot(m,so_ln,exp_var = T,return_dat=T,saveplot = F)
  pr %>% mutate(model=n)
})

so.cond.p <- so.cond %>%   
  ggplot(aes(so_ln, exp(est),ymin=ymin,ymax=ymax,
  fill=model
)) +
  geom_line() +
  geom_ribbon(alpha = 0.4) +
  coord_cartesian(expand = F) +
  facet_wrap(~model)+
  labs(x = "Salinity (psu)", y = "eDNA conc.",title="Conditional Effect of Salinity")+
  theme_minimal()
so.cond.p
```

## Using Presence-Absence

Try to use presence/absence to make some nicer covariate plots

```{r}
f3.preds <- predict(f3,newdata=grid.pred) %>% 
  mutate(expest=exp(est))
f3.pa.preds <- predict(f3,newdata=grid.pred,nsim=30,model=1)
f3.preds$pa <- rowMeans(plogis(f3.pa.preds))

f3.preds %>% 
  ggplot(aes(bathy.bottom.depth_ln %>% exp,pa))+
  # geom_point()+
  geom_smooth()+
  xlim(0,1000)+ylim(0,1)+
  labs(y="Probability of Presence",x="Bottom Depth(m)")

f3.preds %>% 
  ggplot(aes(thetao_ln %>% exp,pa))+
  geom_point()+
  geom_smooth()+ylim(0,1)+
  facet_wrap(~depth_cat)+
  labs(y="Probability of Presence",x="Temperature (C)")
```


# Index of Abundance

Let's just sum up the predictions by year to create an index of abundance. A simple sum should work relatively well here since we are using a regular prediction grid (i.e., all the spatial cells are the same area).

## Fit 3

```{r}
f3_abun <- make_abun_index(f3)+theme(axis.text=element_text(size=12))
ggsave(here('plots','Fit 3 Abundance Index.png'),f3_abun,w=6,h=5,bg = "white")
```


# Make Maps

We can make maps of predicted eulachon eDNA with these models.

## Covariates

Can also make maps of the covariates, just so we know what we're working with
```{r}
thetao_plot <- make_map(grid.pred,thetao)+
    scale_fill_viridis(option="C")+
  labs(title="Temperature")
so_plot <- make_map(grid.pred,so)+
    scale_fill_viridis(option="C",direction = -1)+
  labs(title="Salinity")
thetao_plot
so_plot
```


```{r}
# if we want to zoom in on particular regions
gpsf <- grid.pred %>% st_as_sf(coords=c('x','y'),crs=pred.crs,remove=F)

# Oregon
orbbox <- coastcrop %>% filter(name=="Oregon") %>% st_bbox() %>% .[c(2,4)]
orbbox <- gpsf %>% filter(y>orbbox['ymin'],y<orbbox['ymax']) %>% st_bbox()

# Washington
wabbox <- coastcrop %>% filter(name=="Washington") %>% st_bbox()%>% .[c(2,4)]
wabbox <- gpsf %>% filter(y>wabbox['ymin'],y<wabbox['ymax']) %>% st_bbox()


# Washington and Oregon
wobbox <- coastcrop %>% filter(name%in%c("Washington","Oregon")) %>% st_bbox()%>% .[c(2,4)]
wobbox <- gpsf %>% filter(y>wobbox['ymin'],y<wobbox['ymax']) %>% st_bbox()

# california
cabbox <- coastcrop %>% filter(name=="California") %>% st_bbox()%>% .[c(2,4)]
cabbox <- gpsf %>% filter(y>cabbox['ymin'],y<cabbox['ymax']) %>% st_bbox()
```

## Fit 3

Try this with Fit 3, which was one of the better-looking models: 

```{r,fig.height=8,fig.width=8}
f3.preds <- predict(f3,newdata=grid.pred) %>% 
  mutate(expest=exp(est))

make_map(f3.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 3",fill="ln(eDNA)",x="",y="")+
  theme(axis.text.x=element_blank())
ggsave(here('plots','Fit 3 coastwide.png'),w=6,h=5,bg='white')

make_map(f3.preds,est_non_rf)+ #tidy(f3,'fixed')
    scale_fill_viridis(option="C")+
  labs(title="Fit 3: Fixed Effects Only",fill="ln(eDNA)")

make_map(f3.preds,est_rf)+ #tidy(f3,'ran_pars')
    scale_fill_viridis(option="C")+
  labs(title="Fit 3: Random Effects Only",fill="ln(eDNA)")

make_map_bathy(f3.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 3",fill="ln(eDNA)")
make_map_bathy(f3.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 3",fill="ln(eDNA)",x="",y="")+
  ylim(orbbox[c(2,4)])+xlim(orbbox[c(1,3)])
ggsave(here('plots','Fit 3 Oregon.png'),w=6,h=5,bg="white")
make_map_bathy(f3.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 3",fill="ln(eDNA)",x="",y="")+
  ylim(wabbox[c(2,4)])+xlim(wabbox[c(1,3)])
ggsave(here('plots','Fit 3 Washington.png'),w=6,h=5,bg="white")

make_map_bathy(f3.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="",fill="ln(eDNA)",x="",y="")+
  ylim(wobbox[c(2,4)])+xlim(wobbox[c(1,3)])+
  coord_sf(datum=NA)
ggsave(here('plots','Fit 3 WA and OR.png'),w=6,h=8,bg="white")

make_map_bathy(f3.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 3",fill="ln(eDNA)",x="",y="")+
  ylim(cabbox[c(2,4)])+xlim(cabbox[c(1,3)])
ggsave(here('plots','Fit 3 California.png'),w=6,h=5,bg="white")

# quantiles?
f3.preds %>% 
  arrange(est) %>% 
  mutate(quantest=ntile(est,100)) %>% 
  mutate(quantcut=cut(quantest,10)) %>% 
  mutate(top5=ifelse(quantest>=95,T,F)) %>% 
  make_map(top5)+
  scale_fill_manual(values=c('gray20','#BD3786FF'))+
  # scale_fill_viridis_c()+
    # scale_fill_manual(breaks=seq(0,1,by=0.1),values=viridis_pal(option="C")(11))+
  labs(title="Fit 3",fill="Top 5% (eDNA)")

f3.preds %>% 
  arrange(est) %>% 
  mutate(quantest=ntile(est,100)) %>% 
  mutate(quantcut=cut(quantest,10)) %>% 
  make_map(quantcut)+
  scale_fill_viridis(option='C',discrete=T)+
    # scale_fill_manual(breaks=seq(0,1,by=0.1),values=viridis_pal(option="C")(11))+
  labs(title="Fit 3",fill="Quantile (eDNA)")

#presence/absence
f3.pa.map <- make_presence_absence_map(f3)[[2]]+labs(x="",y="")+
  ylim(wobbox[c(2,4)])+xlim(wobbox[c(1,3)])
ggsave(here('plots','Fit 3 presence absence.png'),w=6,h=5,bg="white")
```

## Fit 4

Try this with Fit 4, which was one of the better-looking models: 

```{r,fig.height=8,fig.width=8}
f4.preds <- predict(f4,newdata=grid.pred)

make_map(f4.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 4",fill="ln(eDNA)")

# f5.preds <- predict(f4,newdata=grid.pred %>% 
#                       mutate(d1=ifelse(depth_cat==0,1,0),
#                              d2=ifelse(depth_cat==50,1,0),
#                              d3=ifelse(depth_cat==150,1,0)))
make_map_bathy(f4.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 4",fill="ln(eDNA)")
make_map_bathy(f4.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 4",fill="ln(eDNA)")+
  ylim(orbbox[c(2,4)])+xlim(orbbox[c(1,3)])
make_map_bathy(f4.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 4",fill="ln(eDNA)")+
  ylim(wabbox[c(2,4)])+xlim(wabbox[c(1,3)])
make_map_bathy(f4.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 4",fill="ln(eDNA)")+
  ylim(cabbox[c(2,4)])+xlim(cabbox[c(1,3)])
# probability of presence
pa <-make_presence_absence_map(f4)
pa[[1]]
pa[[2]]

# quantiles?
f4.preds %>% 
  arrange(est) %>% 
  mutate(quantest=ntile(est,100)) %>% 
  mutate(top5=ifelse(quantest>=95,T,F)) %>% 
  make_map(top5)+
  scale_fill_manual(values=c('gray20','#BD3786FF'))+
  # scale_fill_viridis_c()+
    # scale_fill_manual(breaks=seq(0,1,by=0.1),values=viridis_pal(option="C")(11))+
  labs(title="Fit 4",fill="Top 5% (eDNA)")
```
## Fit 5

Try this with Fit 5, which was one of the better-looking models: 

```{r,fig.height=8,fig.width=8}
f5.preds <- predict(f5,newdata=grid.pred)

make_map(f5.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 5",fill="ln(eDNA)")

make_map_bathy(f5.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 5",fill="ln(eDNA)")
make_map_bathy(f5.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 5",fill="ln(eDNA)")+
  ylim(orbbox[c(2,4)])+xlim(orbbox[c(1,3)])
make_map_bathy(f5.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 5",fill="ln(eDNA)")+
  ylim(wabbox[c(2,4)])+xlim(wabbox[c(1,3)])
make_map_bathy(f5.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 5",fill="ln(eDNA)")+
  ylim(cabbox[c(2,4)])+xlim(cabbox[c(1,3)])
# probability of presence
pa <-make_presence_absence_map(f5)
pa[[1]]
pa[[2]]

# quantiles?
f5.preds %>% 
  arrange(est) %>% 
  mutate(quantest=ntile(est,100)) %>% 
  mutate(top5=ifelse(quantest>=95,T,F)) %>% 
  make_map(top5)+
  scale_fill_manual(values=c('gray20','#BD3786FF'))+
  # scale_fill_viridis_c()+
    # scale_fill_manual(breaks=seq(0,1,by=0.1),values=viridis_pal(option="C")(11))+
  labs(title="Fit 5",fill="Top 5% (eDNA)")
```

## Fit 7.2

Try this with Fit 7.2, which was one of the better-looking models: 

```{r,fig.height=8,fig.width=8}
f7.2.preds <- predict(f7.2,newdata=grid.pred)

make_map(f7.2.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 7.2",fill="ln(eDNA)")

# f5.preds <- predict(f7.2,newdata=grid.pred %>% 
#                       mutate(d1=ifelse(depth_cat==0,1,0),
#                              d2=ifelse(depth_cat==50,1,0),
#                              d3=ifelse(depth_cat==150,1,0)))
make_map_bathy(f7.2.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 7.2",fill="ln(eDNA)")
make_map_bathy(f7.2.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 7.2",fill="ln(eDNA)")+
  ylim(orbbox[c(2,4)])+xlim(orbbox[c(1,3)])
make_map_bathy(f7.2.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 7.2",fill="ln(eDNA)")+
  ylim(wabbox[c(2,4)])+xlim(wabbox[c(1,3)])
make_map_bathy(f7.2.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Fit 7.2",fill="ln(eDNA)")+
  ylim(cabbox[c(2,4)])+xlim(cabbox[c(1,3)])

# quantiles?
f7.2.preds %>% 
  arrange(est) %>% 
  mutate(quantest=ntile(est,100)) %>% 
  mutate(top5=ifelse(quantest>=95,T,F)) %>% 
  make_map(top5)+
  scale_fill_manual(values=c('gray20','#BD3786FF'))+
  # scale_fill_viridis_c()+
    # scale_fill_manual(breaks=seq(0,1,by=0.1),values=viridis_pal(option="C")(11))+
  labs(title="Fit 7.2",fill="Top 5% (eDNA)")
```

# Test Mesh Complexity

One thing that seems to have an effect on our fits is the configuration of the INLA mesh, particularly its complexity, or number of knots. Meshes that are too complex can make pretty maps, but we run the risk of overfitting. As a general rule of thumb, we want to pick a less complex mesh, but one that still allows us to make good predictions. Let's see if we can use cross-validation likelihoods to assess which mesh to use.

```{r}
# we are using a combined INLA/sdmTMB workflow to make these meshes. We will vary the "cutoff", which determines the minimum triangle edge length, and in practice determines the number of vertices in the mesh
locs <- d_obs_filt %>%
  distinct(year,station,utm.lon.km,utm.lat.km) %>%
  st_as_sf(coords=c("utm.lon.km", "utm.lat.km"))

max.edge = diff(range(st_coordinates(locs)[,1]))/4
bound.outer = diff(range(st_coordinates(locs)[,1]))/3

domain <-fmesher::fm_nonconvex_hull(locs,
                              concave = -0.025,
                              convex = -0.025)
make_test_mesh <- function(test_cutoff){
  testmesh <- fmesher::fm_mesh_2d_inla(
    #loc=locs[,c("utm.lon","utm.lat")],
    loc.domain = domain, # coordinates
    boundary=domain,
    max.edge = c(max.edge,max.edge*10), # max triangle edge length; inner and outer meshes
    offset = c(max.edge, bound.outer),  # inner and outer border widths
    #max.n.strict=100,#,
    min.angle=15,
    cutoff = test_cutoff # minimum triangle edge length
    )
  make_mesh(d_obs_filt,c("utm.lon.km","utm.lat.km"),mesh=testmesh)
}

# let's make a series of meshes of increasing complexity, then fit our model and test its likelihood
cutoffs_to_try <- max.edge/seq(1.5,8,by=0.5)
meshes <- map(cutoffs_to_try,make_test_mesh)
# knots_to_try <- seq(50,175,by=25)
# meshes <- map(knots_to_try,~make_mesh(d_obs_filt,c("utm.lon.km", "utm.lat.km"),n_knots=.x))
map(meshes,plot)
```

Now try to fit the same model on each mesh

```{r}
clust <-sample(seq_len(3), size = nrow(d_obs_filt), replace = TRUE)
tic("CV mesh analysis")
meshes.ll <- map_dbl(meshes,function(m){
  fit <- sdmTMB_cv(
    Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+washed+(1|yr_fct),
    data = d_obs_filt,
    mesh = m,
    offset= "offsets_all",
    spatial="on",
    fold_ids=clust,
    family = stdcurve(),
    control = sdmTMBcontrol(stdcurve_df = d)
  )
  fit$sum_loglik
})
toc()

```

Plot the relationship between mesh complexity and the 3-fold CV likelihood. Knots is one way to measure, but there's also total vertices (`mesh$mesh$n`)

```{r}
mesh.comparison <- tibble(cutoff=cutoffs_to_try,sumll=meshes.ll) %>% 
  ggplot(aes(cutoff,sumll))+
  geom_line()+
  geom_point()+
  # stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1,se=F)+
  geom_smooth(se=F)+
  labs(x="Minimum Interior Edge Length",y="Summed Log Likelihood")
mesh.comparison

mesh.comparison2 <- tibble(cutoff=cutoffs_to_try,sumll=meshes.ll) %>% 
  mutate(nverts=map_dbl(meshes,~.$mesh$n)) %>% 
  ggplot(aes(nverts,sumll))+
  geom_line()+
  geom_point()+
  geom_smooth(se=F)+
  # stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1,se=F)+
  scale_x_continuous(limits=c(100,400),breaks=seq(100,400,by=100))+
  labs(x="Number of Vertices",y="Summed Log Likelihood")
mesh.comparison2

meshes_table <- tibble(cutoff=cutoffs_to_try,sumll=meshes.ll) %>% 
  mutate(nverts=map_dbl(meshes,~.$mesh$n))

meshes.edges.nverts <- meshes_table%>% 
  ggplot(aes(cutoffs_to_try,nverts))+
  # geom_line()+
  geom_point()+
  labs(y="Number of Vertices",x="Minimum Interior Edge Length")

meshes.p <-plot_grid(mesh.comparison,mesh.comparison2,nrow=1)

meshes.edges.nverts
meshes.p

ggsave(here('plots','mesh_complexity_test.png'),meshes.p,h=4,w=7,bg='white')
```

Looks like a mesh somewhere in the middle may be the sweet spot! There are two mini-peaks, around cutoff edge lengths of either 19 or 29. By default, let's pick the simpler/coarser of these two options (~29km minimum edge length, 187 vertices). We can work this back through the code above.


# Scratch

```{r,fig.height=8,fig.width=8}
test <- predict(f5,newdata=grid.pred %>% 
                      mutate(d1=ifelse(depth_cat==0,1,0),
                             d2=ifelse(depth_cat==50,1,0),
                             d3=ifelse(depth_cat==150,1,0)))

# maps of SVCs by depth
make_map(test %>% filter(depth_cat==0,year==2019),zeta_s_d1)+
  scale_fill_viridis(option="C")+
  ggtitle("Spatial Factor: Surface")

make_map(test %>% filter(depth_cat==0,year==2019),zeta_s_d2)+
  scale_fill_viridis(option="C")+ggtitle("Spatial Factor: 50m Depth")

make_map(test %>% filter(depth_cat==0,year==2019),zeta_s_d3)+
  scale_fill_viridis(option="C")+ggtitle("Spatial Factor: 150m Depth")

# maps of all spatial random effects (omega_s)
make_map(test %>% filter(depth_cat==0,year==2019),omega_s)+
  scale_fill_viridis(option="C")+
  ggtitle("Spatial Random Effects")

# fixed effects only
make_map(test,est_non_rf)+
  scale_fill_viridis(option="C")+
  ggtitle("Prediction: Fixed Effects Only")

# random effects only prediction
make_map(test,est_rf)+
  scale_fill_viridis(option="C")+
  ggtitle("Prediction: Random Effects Only")
```

## Trying sdmTMB_cv

Model comparison

```{r}
clust <-sample(seq_len(3), size = nrow(d_obs_filt), replace = TRUE)
f3cv <- sdmTMB_cv(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)

f4cv <- sdmTMB_cv(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+washed+(1|depth_cat)+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)

f5cv <- sdmTMB_cv(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial_varying= ~d1+d2+d3,
  spatiotemporal="off",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)

f3cv$sum_loglik
f4cv$sum_loglik
f5cv$sum_loglik
# higher is better
# in this case, Fit 3 seems to have the greatest support with likelihood
```

## Other Formulas

```{r}
d_obs_filt_test <- d_obs_filt %>% 
  mutate(bottomT_ln=log(bottomT))
ft <- sdmTMB_cv(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+uo+vo+so_ln+s(bottomT_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt_test,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
ft$sum_loglik
m1 <- ft$models[[1]]

make_pred_obs_plots(ft$models[[1]],d,saveplots = F)
# make_cond_plot(ft,hour,saveplot = F)
make_cond_plot(m1,bathy.bottom.depth_ln,exp_var=T,saveplot = F)+xlim(0,500)
make_cond_plot(m1,thetao_ln,exp_var=T,saveplot = F)
make_cond_plot(m1,so_ln,exp_var=T,saveplot = F)
make_cond_plot(m1,vo,exp_var=F,saveplot = F)
make_cond_plot(m1,uo,exp_var=F,saveplot = F)

make_cond_plot(m1,bottomT_ln,exp_var=T,saveplot = F)
# without so_ln: -4209.852
# with uo, vo, and so_ln: sumloglik=-4195.278
# with just uo and so_ln: -4195.992
# with just uo and s(so_ln,k=3): -4219.442
# with s() for uo,vo,and so: -4279.387
# adding krill seems to make the likelihood decrease

# with log(bottom temperature), uo,vo, and so, but NOT thetao: -4273.88
```

```{r}
tic("Fitting model: ")
d_obs_filt$depth_cat <- factor(d_obs_filt$depth_cat)
ft <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(so_anom,k=3)+s(thetao_anom,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  offset = 'offsets_all',
  mesh = mesh,
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
ft
#AIC 13463.99
# make_pred_obs_plots(f2,d,model_name="Fit2")
make_cond_plot(ft,bathy.bottom.depth_ln,exp_var=T,saveplot=F)
make_cond_plot(ft,thetao_anom,exp_var=F,saveplot=F)
```

```{r}
tic("Fitting model: ")
ft <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial_varying= ~d1+d2+d3,
  spatial="off",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
ft
#AIC 13331.86
make_pred_obs_plots(ft,d,model_name="Test")
make_cond_plot(ft,thetao_ln,exp_var=T,saveplot=F)+labs(x="Temperature (C)")
make_cond_plot(ft,bathy.bottom.depth_ln,exp_var=T,saveplot=F)+labs(x="Bottom Depth (m)")
make_cond_plot(ft,so_ln,exp_var=T,saveplot=F)+labs(x="Salinity (psu)")
```
Smooth by factor (depth).

```{r}
fs <- sdmTMB(
  Ct ~ 0+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+so_ln+s(chl_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
fs
make_pred_obs_plots(fs,d,model_name="Test")
make_cond_plot(fs,thetao_ln,exp_var=T,saveplot=F)+labs(x="Temperature (C)")
make_cond_plot(fs,bathy.bottom.depth_ln,exp_var=T,saveplot=F)+labs(x="Bottom Depth (m)")
make_cond_plot(fs,chl_ln,exp_var=T,saveplot=F)+labs(x="Chl")
```

