---
title: "Eulachon eDNA SDMs"
author: "Owen R. Liu"
date: "2024-02-09"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
options(dplyr.summarise.inform=FALSE)
library(tidyverse)
library(here)
library(tictoc)
library(sf)
library(viridis)
library(ggsci)
library(cowplot)
library(rnaturalearth)
library(marmap)
library(RANN)
library(contoureR)
library(sdmTMB)
library(nngeo)
library(contoureR)
library(corrplot)
```

# Purpose

Build species distribution models in `sdmTMB` for eulachon, based on spatiotemporal data on eDNA for eulachon (*Thaleichthys pacificus*). The data are structured in three dimensions (latitude, longitude, depth), or four including year, and so we want to build a flexible set of models that can take advantage of this dimensionality of the data to create high-resolution predictions.

We will use environmental covariates to constrain our predictions, based on temperature, salinity, and bottom depth, as well as biological productivity variables. In spatiotemporal models, though, we also need to decide how to guide the model to partition variation across different spatial and depth fields (i.e., the spatially autocorrelated parts of the model that are estimated in parallel to the fitted covariates). To explore these options, we decided to try models with different combinations of spatial fields and intercepts:

Spatial field options to try: 

*   One common spatial field across all data
*   spatial field by depth
*   spatial field by year
*   spatial field by depth and year

Intercepts options to try

*   single intercept
*   random intercept by depth category
*   random intercept by year
*   random intercept by depth and year

Combinations of spatial fields and intercepts gives us 16 models to consider. Some of these models will certainly break and/or not converge, particularly the more complex versions.

# Pre-processing

Import eDNA and covariate data, designate offsets, and build prediction grid. Note that cleaning and joining of the standards and unknown samples has been done in other scripts---one to process the 2019 data, one for 2021, and one to join them. If desired, you can re-run those here, but they are not evaluated out for now to save time.

```{r,eval=F,warning=F,message=F}
rmarkdown::render(here::here('scripts','process 2019 eulachon data.Rmd'),quiet=TRUE)
rmarkdown::render(here::here('scripts','process 2021 eulachon data.Rmd'),quiet=TRUE)
source(here::here('scripts','join_eulachon_qPCR_data.R'))

rm(list=ls())
```

## Import Data

Spatial coordinate reference system we will use for thesse analyses:

```{r}
pred.crs <- terra::rast(here('data','raster_grid_blake','fivekm_grid.tif')) %>% st_crs()
```

Load in our cleaned and joined eDNA data.

```{r}
# qPCR standards
d <- read_rds(here('data','qPCR',"eulachon qPCR 2019 and 2021 standards clean.rds")) %>% 
  mutate(Ct=replace_na(Ct,0))# delta-models in sdmTMB need this

# field samples (already joined to physical GLORYS data)
d_obs <- read_rds(here('data','eDNA_glorys_matched.rds'))
d_obs_bgc <- read_rds(here('data','eDNA_glorys_bgc_matched.rds')) %>% 
  dplyr::select(chl:phyc)
# river influence variable, already matched to eDNA
river_metric <- read_rds(here('data','eDNA_river_influence_matched.rds')) %>% 
  dplyr::select(river_input)

d_obs <- d_obs %>% 
  bind_cols(d_obs_bgc) %>%
  bind_cols(river_metric) %>% 
  mutate(Ct=replace_na(Ct,0)) %>% # delta-models in sdmTMB need this
  mutate(utm.lon.km=utm.lon.m/1000,
         utm.lat.km=utm.lat.m/1000)

# positive samples by depth/year
d_obs %>% 
  filter(Ct>0) %>% 
  count(year,depth_cat) %>% 
  ggplot(aes(depth_cat,n,fill=factor(year)))+
  labs(x="Depth Category",y="Number of Positive Observations",fill="Year")+
  geom_col(position='dodge')
# table of samples by depth/year
obs_tbl <- d_obs %>% 
  mutate(amp=ifelse(Ct>0,"Amplified","Did not amplify")) %>% 
  # filter(depth_cat %in% c(0,50,150)) %>%
  mutate(depth_cat=factor(depth_cat)) %>% 
  count(year,depth_cat,amp)
# positive samples by depth/year
d_obs %>% 
  mutate(amp=ifelse(Ct>0,"Amplified","Did not amplify")) %>% 
  # filter(depth_cat %in% c(0,50,150)) %>% 
  mutate(depth_cat=factor(depth_cat)) %>% 
  count(year,depth_cat,amp) %>% 
  ggplot(aes(depth_cat,n,fill=factor(amp)))+
  labs(x="Depth Category",y="Number of Samples",fill="Amplified?")+
  facet_wrap(~year,nrow=1)+
  scale_fill_manual(values=PNWColors::pnw_palette("Starfish",2))+
  geom_col()+theme(axis.text=element_text(size=12),axis.title = element_text(size=12))

# field samples, filtered such that we are only including depth categories 0 (surface), 50m, and 150m
d_obs_filt <- d_obs %>% filter(depth_cat %in% c(0,50,150)) 

# %>% 
#   # what if we decide to use only samples with spatial coverage in both years? Basically all samples north of 37.5
#   filter(lat>37.5)

d_obs_sf <- d_obs_filt %>% st_as_sf(coords=c('utm.lon.m','utm.lat.m'),crs=pred.crs)
```

## Covariate data

Bathymetry/bottom depth was already attached to the samples during pre-processing. The field samples were also joined to modeled GLORYS data from their [Global Ocean Physics Reanalysis](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description). See script `match_GLORYS_temp_salinity.R`.

### Krill/Euphausiid data

These data on the relative abundance of krill come from Beth Phillips, and are organized in the script `krill_nasc_matching`. As with the raw data cleaning above, we do not evaluate this chunk here and just use the output; but the user can run it if they wish. Be warned that it will take awhile to run.

```{r,eval=F}
rmarkdown::render(here::here('scripts','krill_nasc_matching.Rmd'),quiet=TRUE)
```

```{r}
# Load krill data
edna_krill_metrics <- read_rds(here('model output','edna_krill_metrics.rds')) %>% 
  filter(depth_cat %in% c(0,50,150))
  
d_obs_filt <- d_obs_filt %>% bind_cols(edna_krill_metrics %>% dplyr::select(k1:k7))
```

### River discharge data

We also have a dataset of mean river discharge for US West Coast rivers, which we have projected onto our ocean model grid using exponential distance weighting (see script `river_discharge`). Basically, this allows us to explore a variable representing the distance of each spatial grid cell to river mouths, weighted by distance between river and grid cell, as well as by the magnitude of annual discharge from the river.

```{r}
river_mean_discharge <- read_rds(here('data','rivers_mean_discharge_MarMay.rds')) # mean March to May discharge for each west coast river
riverine_influence <- read_rds(here('data','river_influence_grid_matched.rds')) # grid matched
```

## Log Covariates

Try natural logging covariates to scale them better for fitting

```{r}
#log covariates
d_obs_filt <- d_obs_filt %>% 
  # make bottom depth positive
  mutate(bathy.bottom.depth=-bathy.bottom.depth) %>% 
  # log covariates
  mutate(across(c(so,thetao,bathy.bottom.depth,k1,k2,k3,k4,k5,k6,k7,chl,no3,nppv,o2,phyc,river_input),list(ln=function(x){
    x[x==0]<-1
    log(x)
    })))
```

```{r}
logged_vars_p <- d_obs_filt %>% 
  select(contains("_ln")) %>% 
  pivot_longer(everything(),names_to="variable",values_to="val") %>% 
  ggplot(aes(val,fill=variable))+
  geom_density()+
  facet_wrap(~variable,scales='free')
logged_vars_p
```

## Other Transformations

Here we add some other transformations of the above variables in case we want to try them in fitting

```{r}
d_obs_filt <- d_obs_filt %>% 
  # squares
  mutate(depth_ln2=bathy.bottom.depth_ln^2) %>% 
  mutate(thetao_ln2=thetao_ln^2,
         so_ln2=so_ln^2)

# defining factors for depth category and year
d_obs_filt$depth_cat <- factor(d_obs_filt$depth_cat)

d_obs_filt$yr_fct <- as.factor(d_obs_filt$year)
```

## Designate Offsets

While we are primarily interested in eulachon DNA concentration over space, time, and depth, we do not directly measure this concentration with our observations. Rather, we have replicate qPCR observations assocated with a water sample taken from a Niskin bottle and we have to account for the various modifications that have occurred during water sampling and processing. As part of this, we have a series of offsets that modify the true DNA concentration to affect what we observed in the qPCR. These [offsets](https://pbs-assess.github.io/sdmTMB/articles/model-description.html#offset-terms) will go into the model as log-transformed variables (because we will use a log link), and are scaling factors without estimated coefficients.

We are primarily concerned with three offsets: one for the volume filtered out of each 2.5L Niskin bottle for each sample (`ln_vol_offset`). Second, some samples were inhibited in PCR, and were diluted to eliminate this inhibition (`ln_dil_offset`). Third, there was a wash error with some samples in 2019 that we correct for with a fixed effect.

Finally, we include an expansion factor of 20 to convert our estimates to the standard measure of eDNA copies/L. We derived this assuming a 2.5L water sample eluted in 100uL Longmire and 2 uL used in each PCR reaction. Therfore, each 2uL of sample corresponds to 2% (2uL of 100uL) of the total sample and therefore is equivalent to the copies DNA contained in 50mL of water. (0.02 x 2500 ml = 50 ml). So, multiplying by 20 gets us to copies/L.

## INLA Mesh

Create the mesh that will be the basis of our estimation---based on the spatial distribution of our sample data, the mesh is created in order to facilitate efficient processing and estimatino of the model in sdmTMB. Instead of calculating enormous covariance matrices in model fitting, we can drastically reduce computation time by using the mesh.

NOTE: SEE THE BOTTOM OF THIS SCRIPT FOR SOME DIAGNOSTICS ON MESH COMPLEXITY

```{r}
locs <- d_obs_filt %>%
  distinct(year,station,utm.lon.km,utm.lat.km) %>%
  st_as_sf(coords=c("utm.lon.km", "utm.lat.km"))

max.edge = diff(range(st_coordinates(locs)[,1]))/4
bound.outer = diff(range(st_coordinates(locs)[,1]))/3

domain <-fmesher::fm_nonconvex_hull(locs,
                              concave = -0.025,
                              convex = -0.025)
# I think we can use this to subset our prediction grid

inla_mesh <- fmesher::fm_mesh_2d_inla(
    #loc=locs[,c("utm.lon","utm.lat")],
    loc.domain = domain, # coordinates
    boundary=domain,
    max.edge = c(max.edge,max.edge*10), # max triangle edge length; inner and outer meshes
    offset = c(max.edge, bound.outer),  # inner and outer border widths
    #max.n.strict=100,#,
    min.angle=15,
    cutoff = 29.32207 # minimum triangle edge length (see mesh selection exercise below)
    )

mesh <- make_mesh(d_obs_filt,c("utm.lon.km", "utm.lat.km"),mesh=inla_mesh) # this was decided based on a cross-validation exercise (see "Test Mesh Complexity" below)

png(here('plots','selected_inla_mesh.png'),w=480,h=800,bg="transparent")
plot(mesh)
dev.off()
```

```{r}
# as of 2/13/24, these were the mesh options setup that Ole is using for hake
# locs <- d_obs_filt %>%
#   distinct(year,station,utm.lon.km,utm.lat.km) %>% 
#   st_as_sf(coords=c("utm.lon.km", "utm.lat.km"))
# domain <-fmesher::fm_nonconvex_hull(locs,
#                               concave = -0.025,
#                               convex = -0.025)
# 
# inla_mesh <- fmesher::fm_mesh_2d_inla(
#   #loc=locs[,c("utm.lon","utm.lat")],
#   loc.domain = domain, # coordinates
#   boundary=domain,
#   max.edge = c(40, 1000), # max triangle edge length; inner and outer meshes
#   offset = c(30, 80),  # inner and outer border widths
#   #max.n.strict=100,#,
#   cutoff = 64 , # minimum triangle edge length
#   min.angle=20
# )


# 
# mesh <- make_mesh(d_obs_filt, c("utm.lon.km", "utm.lat.km"), mesh = inla_mesh)
# mesh2 <- make_mesh(d_obs_filt, c("utm.lon.km", "utm.lat.km"), cutoff=20)
# mesh$mesh$n
# mesh2$mesh$n
# plot(mesh)
# plot(mesh2)
```

Now we are ready to start fitting models. We do this in two stages: first, we compare model structures to pick an appropriate structural format for sdmTMB given our data. Then, we use variations on our chosen model to choose appropriate environmental covariates to choose.

# Fit Models

In the first stage of model selection, we decide on an appropriate model structure, referring to our choices about specifications of intercepts and type of latent spatial field. All models in this initial stage will include a spatial field of some flavor, as well as penalized splines for temperature and depth covariates. They will vary structurally in the specification of the spatial field(s) and the types of offsets/intercepts we impose.

Spatial field options to try: 

*   One common spatial field across all data
*   spatial field by depth
*   spatial field by year
*   spatial field by depth and year

Intercepts options to try

*   single intercept
*   random intercept by depth category
*   random intercept by year
*   random intercept by depth and year

We will do this one at a time, increasing slowly in complexity. In the second stage, we try different environmental covariates

As a measure of model fit, we use 3-fold cross-validation with replacement. Using the same folds across all candidate models, we can pull the summed log likelihood from cross-validation as a measure of model skill.

## Cross Validation Folds

```{r}
set.seed(8913)
# clust <-sample(seq_len(3), size = nrow(d_obs_filt), replace = TRUE)
# clust <-as.integer(d_obs_filt$year==2019)+1# by year??
# maybe do this by year and station, so we don't remove huge spatial blocks
# hmm, and maybe we need an unbalanced sample.
clust <- d_obs_filt %>% group_by(year,station) %>% 
  mutate(cvclust=sample(seq_len(2),size=n(),replace=T)) %>% 
  ungroup()

clust %>% 
  ggplot(aes(utm.lon.m,utm.lat.m,color=factor(cvclust)))+
  geom_point()+
  facet_grid(year~depth_cat)+
  coord_equal()
  
clust <- clust %>% pull(cvclust)
```


```{r}
# source the plotting utilities/helpers
source(here("scripts","plotting_utils.R"))
```


## Common Spatial Field

```{r}
tic("Fitting:")
f0 <- sdmTMB(
  Ct ~ 1+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f0
paste("AIC:",AIC(f0))
make_pred_obs_plots(f0,d,model_name="Fit0")

tic("Calculating 3-fold cross-validation:")
f0_cv <- sdmTMB_cv(
  Ct ~ 1+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f0_cv$sum_loglik)
```

First, a model with just one common spatial field, and a single intercept.

```{r}
tic("Fitting:")
f1 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f1
paste("AIC:",AIC(f1))
make_pred_obs_plots(f1,d,model_name="Fit1")

tic("Calculating 3-fold cross-validation:")
f1_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f1_cv$sum_loglik)

make_cond_plot(f1,bathy.bottom.depth_ln,exp_var=T,saveplot=F)
```

One spatial field, random intercept by depth category.

```{r}
tic("Fitting:")
f2 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f2
paste("AIC:",AIC(f2))
make_pred_obs_plots(f2,d,model_name="Fit2")

tic("Calculating 3-fold cross-validation:")
f2_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f2_cv$sum_loglik)

make_cond_plot(f2,bathy.bottom.depth_ln,exp_var=T,saveplot=F)

```

One spatial field, random intercept by year.

```{r}
tic("Fitting:")
f3 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f3
paste("AIC:",AIC(f3))
make_pred_obs_plots(f3,d,model_name="Fit3")

tic("Calculating 3-fold cross-validation:")
f3_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f3_cv$sum_loglik)

make_cond_plot(f3,bathy.bottom.depth_ln,exp_var=T,saveplot=F)
```

```{r}
# make_pred_obs_plots(f3,d,model_name="Fit3")
# make_cond_plot(f3,thetao_ln,exp_var=T,saveplot=F)+labs(x="Temperature (C)",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
# make_cond_plot(f3,bathy.bottom.depth_ln,exp_var=T,saveplot=F)+labs(x="Bottom Depth (m)",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
# make_cond_plot(f3,so_ln,exp_var=T,saveplot=F)+labs(x="Salinity (psu)")
# make_cond_plot(f3,k7_ln,exp_var=T,saveplot=F)+labs(x="Krill KDE",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
# make_cond_plot(f3,nppv_ln,exp_var=T,saveplot=F)+labs(x="NPPV",title="")+
#   theme(axis.text=element_text(size=14),axis.title=element_text(size=16))
```


```{r,fig.height=8,fig.width=6}
# f3_abun_lat <- make_lat_abundance(f3)+theme(text=element_text(size=14))
# ggsave(here('plots','f3_abun_by_lat.png'),f3_abun_lat,h=8,w=6,bg='white')
```

One spatial field, random intercept by year and depth category

```{r}
tic("Fitting:")
f4 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f4
paste("AIC:",AIC(f4))
make_pred_obs_plots(f4,d,model_name="Fit4")

tic("Calculating 3-fold cross-validation:")
f4_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f4_cv$sum_loglik)
```

## Spatial Field by Depth

Spatial field by depth category, single intercept.

```{r}
# make dummy factors for depth categories
m <- model.matrix(Ct ~ depth_cat, data = d_obs_filt)
d_obs_filt$d1 <- m[,1]
d_obs_filt$d2 <- m[,2]
d_obs_filt$d3 <- m[,3]
```
 
```{r}
tic("Fitting:")
f5 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  spatial_varying= ~d1+d2+d3,
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f5
paste("AIC:",AIC(f5))
make_pred_obs_plots(f5,d,model_name="Fit5")

tic("Calculating 3-fold cross-validation:")
f5_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatial_varying= ~d1+d2+d3,
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f5_cv$sum_loglik)
# had difficulty partitioning between sigma_o (spatial field) and sigma_z (spatially varying coefficients)

```

Spatial field by depth category, random intercept by depth category. This one feels weird

```{r}
tic("Fitting:")
f6 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  spatial_varying= ~d1+d2+d3,
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f6
paste("AIC:",AIC(f6))
make_pred_obs_plots(f6,d,model_name="Fit6")

tic("Calculating 3-fold cross-validation:")
f6_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatial_varying= ~d1+d2+d3,
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f6_cv$sum_loglik)
```

Spatial field by depth category, random intercept by year.

```{r}
tic("Fitting:")
f7 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  spatial_varying= ~d1+d2+d3,
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f7
paste("AIC:",AIC(f7))
make_pred_obs_plots(f7,d,model_name="Fit7")

tic("Calculating 3-fold cross-validation:")
f7_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatial_varying= ~d1+d2+d3,
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f7_cv$sum_loglik)
```


Spatial field by depth category, random intercept by depth and year

```{r}
tic("Fitting:")
f8 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  spatial_varying= ~d1+d2+d3,
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f8
paste("AIC:",AIC(f8))
make_pred_obs_plots(f8,d,model_name="Fit8")

tic("Calculating 3-fold cross-validation:")
f8_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  spatial="on",
  spatial_varying= ~d1+d2+d3,
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f8_cv$sum_loglik)
```

## Spatial Field by Year

Spatial field by year, single intercept.

```{r}
tic("Fitting: ")
f9 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f9

paste("AIC:",AIC(f9))
make_pred_obs_plots(f9,d,model_name="Fit9")

tic("Calculating 3-fold cross-validation:")
f9_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f9_cv$sum_loglik)
```

Spatial field by year, random intercept by depth category.

```{r}
tic("Fitting: ")
f10 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f10

paste("AIC:",AIC(f10))
make_pred_obs_plots(f10,d,model_name="Fit10")

tic("Calculating 3-fold cross-validation:")
f10_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f10_cv$sum_loglik)
```

Spatial field by year, random intercept by year.

```{r}
tic("Fitting: ")
f11 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f11

paste("AIC:",AIC(f11))
make_pred_obs_plots(f11,d,model_name="Fit11")

tic("Calculating 3-fold cross-validation:")
f11_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f11_cv$sum_loglik)
```

Spatial field by year, random intercept by depth and year

```{r}
tic("Fitting: ")
f12 <- sdmTMB(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  spatial="on",
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
f12

paste("AIC:",AIC(f12))
make_pred_obs_plots(f12,d,model_name="Fit12")

tic("Calculating 3-fold cross-validation:")
f12_cv <- sdmTMB_cv(
  Ct ~ 1+s(bathy.bottom.depth_ln,k=3)+s(thetao_ln,k=3)+washed+(1|yr_fct)+(1|depth_cat),
  data = d_obs_filt,
  mesh = mesh,
  offset= "offsets_all",
  time="year",
  spatiotemporal="IID",
  fold_ids=clust,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
paste("CVLL:",f12_cv$sum_loglik)
```

Where we're at so far: models 1 through 4 converged with no issues, and relatively quickly. Models 5-8 seemingly had difficulty apportioning variance between the basic spatial field and the spatially varying coefficients for depth. They converged, but with significant warnings (see,e.g., `sanity(f8)`). Models 9-11 had similar issues estimating the spatial factors by year, and Model 12  not appropriately converge.

# Conditional Effects

We can look at the estimated effects of bottom depth, temperature, salinity and krill on our estimates of eulachon eDNA, conditional on all other variable being held at their means. For now, we use a range of models that converged, and whose diagnostic plots above (e.g. standard curve fits) look reasonable.

## Bottom Depth

```{r}
depth.cond <- purrr::map2_df(list(f1,f2,f3,f4,f5),c("f1","f2","f3","f4","f5"),function(m,n){
  pr <- make_cond_plot(m,bathy.bottom.depth_ln,exp_var = T,return_dat=T,saveplot = F)
  pr %>% mutate(model=n)
})

depth.cond.p <- depth.cond %>%   
  ggplot(aes(bathy.bottom.depth_ln, exp(est),ymin=ymin,ymax=ymax,
  fill=model
)) +
  geom_line() +
  geom_ribbon(alpha = 0.4) +
  scale_x_continuous(limits=c(0,800)) +
  coord_cartesian(expand = F) +
  facet_wrap(~model)+
  labs(x = "Bottom Depth (m)", y = "eDNA conc.",title="Conditional Effect of Bottom Depth")
depth.cond.p
```

## Temperature

```{r}
thetao.cond <- purrr::map2_df(list(f1,f2,f3,f4,f5),c("f1","f2","f3","f4","f5"),function(m,n){
  pr <- make_cond_plot(m,thetao_ln,exp_var = F,return_dat=T,saveplot = F)
  pr %>% mutate(model=n)
})

thetao.cond.p <- thetao.cond %>%   
  ggplot(aes(thetao_ln, exp(est),ymin=ymin,ymax=ymax,
  fill=model
)) +
  geom_line() +
  geom_ribbon(alpha = 0.4) +
  coord_cartesian(expand = F) +
  facet_wrap(~model)+
  labs(x = "Temperature (C)", y = "eDNA conc.",title="Conditional Effect of Temperature")
thetao.cond.p
```

# Variable Selection

Now that we have tested the different model structures, we can choose a preferred model and then try different environmental covariates. We have temperature, depth, salinity, and then the krill metrics. We'll choose two of our highest-likelihood cross-validated fitted models for this exercise, which were Model 1 (single intercept and common spatial field across depths and years) and Model 2 (common spatial field, intercept by depth category). Two other, more complex models (Models 10 and 12) had slightly higher log-likelihood, but they had other convergence issues that invalidate them for further testing.

We use 3-fold cross-validation again to evaluate whether to include each variable. We try all combinations of temperature, depth, and salinity, but choose only one krill metric.

## Abiotic variables

```{r}
# Formulas to try (with different variables)
vars_to_test <- c("bathy.bottom.depth_ln","thetao_ln","so_ln","river_input_ln")
var_combs <- list(vars_to_test) %>% append(combn(vars_to_test,3,simplify = F)) %>% 
  append(combn(vars_to_test,2,simplify = F)) %>% 
  append(unlist(vars_to_test))
```

```{r}
test_covariates_f1 <- function(v){
  f <- paste0("Ct~1+washed+",paste0("s(",v,",k=3)") %>% paste(collapse="+"))
  print(f)
  tic("Calculating 3-fold ll:")
  f <- as.formula(f)
  m_cv <- try(sdmTMB_cv(
    formula=f,
    data = d_obs_filt,
    mesh = mesh,
    offset= "offsets_all",
    spatial="on",
    fold_ids=clust,
    family = stdcurve(),
    control = sdmTMBcontrol(stdcurve_df = d)
  ),
  silent=T)
  if(class(m_cv)=='try-error'){
    print(paste('Error.'))
    return(NA_real_)
  } else{
  # print("CVLL:",m_cv$sum_loglik)
    return(m_cv$sum_loglik)
  }
  toc()
}
test_covariates_f2 <- function(v){
  f <- paste0("Ct~1+washed+(1|depth_cat)+",paste0("s(",v,",k=3)") %>% paste(collapse="+"))
  print(f)
  tic("Calculating 3-fold ll:")
  f <- as.formula(f)
  m_cv <- try(sdmTMB_cv(
    formula=f,
    data = d_obs_filt,
    mesh = mesh,
    offset= "offsets_all",
    spatial="on",
    fold_ids=clust,
    family = stdcurve(),
    control = sdmTMBcontrol(stdcurve_df = d)
  ),
  silent=T)
  if(class(m_cv)=='try-error'){
    print(paste('Error.'))
    return(NA_real_)
  } else{
  # print("CVLL:",m_cv$sum_loglik)
    return(m_cv$sum_loglik)
  }
  toc()
}
```

```{r}
# for Model 1
f1_var_ll<-map_dbl(var_combs,test_covariates_f1)
# for Model 2
f2_var_ll<-map_dbl(var_combs,test_covariates_f2)
```

```{r}
vsf1_p1 <- tibble(model_number=1:15,vars=var_combs,llf1=f1_var_ll,llf2=f2_var_ll) %>% 
  pivot_longer(llf1:llf2,names_to="model_type",values_to = "ll") %>% 
  ggplot(aes(model_number,ll,color=model_type))+
  geom_point()+geom_line()+
  geom_hline(yintercept=-4155.9951,linetype=2)+
  labs(x="Model Number",y="Sum LL")
vsf1_p1
```

## Krill Variables

Do the same process as above, but choosing the temperature-only model and trying the krill variables one at a time.

```{r}
# Formulas to try (with different variables)
kvars_to_test <- paste0("k",1:7,"_ln")

kvars_combs <- map(kvars_to_test,~c(.,"thetao_ln"))

f1_kvar_ll<-map_dbl(kvars_combs,test_covariates_f1)
f2_kvar_ll<-map_dbl(kvars_combs,test_covariates_f2)

#plot
vs2_p <- tibble(model_number=1:7,vars=kvars_combs,llf1=f1_kvar_ll,llf2=f2_kvar_ll) %>% 
  pivot_longer(llf1:llf2,names_to="model_type",values_to = "ll") %>% 
  ggplot(aes(model_number,ll,color=model_type))+
  geom_point()+geom_line()+
  geom_hline(yintercept=-4155.9951,linetype=2)+
  labs(x="Model Number",y="Sum LL")
vs2_p
```

These actually improve our cross-validation likelihood values, particularly variables k4 and k5.

# Chosen Model

Here is the final selected model and variables. Fit the model and then make output plots. Using k4 results in a large standard error for the common spatial field (sigma_o), while k5 has no `sanity` issues. So we'll use k5 as the krill variable.

```{r}
tic("Fitting:")
m <- sdmTMB(
  Ct ~ 1+s(thetao_ln,k=3)+s(k5_ln,k=3)+washed,
  data = d_obs_filt,
  mesh = meshestest[[5]],
  offset= "offsets_all",
  spatial="on",
  spatiotemporal = "off",
  time=NULL,
  family = stdcurve(),
  control = sdmTMBcontrol(stdcurve_df = d)
)
toc()
m
make_pred_obs_plots(m,d,model_name="Model 1")
```

## Conditional Plots

Choose temperature and k4; this is what the conditional effects look like in the chosen model.

```{r,fig.height=6,fig.width=4}
mconds_thetao <- make_cond_plot(m,thetao_ln,exp_var=T,saveplot=F)
mconds_k5 <- make_cond_plot(m,k5_ln,exp_var=T,saveplot=F)

mod1_covar_cond_effs <- cowplot::plot_grid(mconds_thetao,mconds_k5,nrow=2)
ggsave(here('plots','Model 1 Conditional Covariate Effcts.png'),mod1_covar_cond_effs,w=4,h=6,bg='white')
```


# Index of Abundance

Let's just sum up the predictions by year to create an index of abundance. A simple sum should work relatively well here since we are using a regular prediction grid (i.e., all the spatial cells are the same area).

```{r}
m_abun <- make_abun_index(m)+theme(axis.text=element_text(size=12))
m_abun_lat <- make_lat_abundance(m,depth_integrate = T)+theme(axis.text=element_text(size=12))+xlim(35,50)

river_disc_p <- river_mean_discharge %>% 
  slice_max(order_by=discharge_mean,n=14) %>% 
  ggplot()+
  geom_col(aes(mouth_lat,discharge_mean))+
  labs(x="Latitude",y="Mean Discharge (m^3/s)",title="River Discharge")+
  coord_flip()+theme(axis.text=element_text(size=12))+xlim(35,50)

river_disc_p
m_abun_lat

lat_abun_river_disc_p <- plot_grid(m_abun_lat,river_disc_p,nrow=1,rel_widths=c(2,1),labels=c("a","b"))

lat_abun_river_disc_p
ggsave(here('plots','Model 1 Abundance Index.png'),m_abun,w=6,h=5,bg = "white")
ggsave(here('plots','Model 1 Abundance Index and River Discharge.png'),lat_abun_river_disc_p,w=8,h=6,bg = "white")
```

# Make Maps

We can make maps of predicted eulachon eDNA with these models.

## Covariates Maps

Can also make maps of the covariates, just so we know what we're working with.

```{r,fig.height=10,fig.width=8}
thetao_plot <- make_map(grid.pred,thetao)+
    scale_fill_viridis(option="C")+
  labs(title="Temperature")
k5_plot <- make_map(grid.pred,k5_ln)+
  scale_fill_viridis(option="C")+
  labs(title="Krill Metric 5 (log)")
thetao_plot
k5_plot

ggsave(here('plots','temperature_by_depth_year_map.png'),thetao_plot,w=6,h=10)
ggsave(here('plots','krill_metric5_by_depth_year_map.png'),k5_plot,w=6,h=10)
```


```{r}
# if we want to zoom in on particular regions
gpsf <- grid.pred %>% st_as_sf(coords=c('x','y'),crs=pred.crs,remove=F)

# Oregon
orbbox <- coastcrop %>% filter(name=="Oregon") %>% st_bbox() %>% .[c(2,4)]
orbbox <- gpsf %>% filter(y>orbbox['ymin'],y<orbbox['ymax']) %>% st_bbox()

# Washington
wabbox <- coastcrop %>% filter(name=="Washington") %>% st_bbox()%>% .[c(2,4)]
wabbox <- gpsf %>% filter(y>wabbox['ymin'],y<wabbox['ymax']) %>% st_bbox()


# Washington and Oregon
wobbox <- coastcrop %>% filter(name%in%c("Washington","Oregon")) %>% st_bbox()%>% .[c(2,4)]
wobbox <- gpsf %>% filter(y>wobbox['ymin'],y<wobbox['ymax']) %>% st_bbox()

# california
cabbox <- coastcrop %>% filter(name=="California") %>% st_bbox()%>% .[c(2,4)]
cabbox <- gpsf %>% filter(y>cabbox['ymin'],y<cabbox['ymax']) %>% st_bbox()
```

## Spatial Predictions

Make spatial predictions with our selected model

```{r,fig.height=8,fig.width=8}
m.preds <- predict(m,newdata=grid.pred) %>% 
  mutate(expest=exp(est))

make_map(m.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Model 1",fill="ln(eDNA)",x="",y="")+
  theme(axis.text.x=element_blank())
ggsave(here('plots','Model 1 coastwide.png'),w=6,h=5,bg='white')

make_map(m.preds,est_non_rf)+ #tidy(m,'fixed')
    scale_fill_viridis(option="C")+
  labs(title="Model 1: Fixed Effects Only",fill="ln(eDNA)")

make_map(m.preds,est_rf)+ #tidy(m,'ran_pars')
    scale_fill_viridis(option="C")+
  labs(title="Model 1: Random Effects Only",fill="ln(eDNA)")

make_map_bathy(m.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Model 1",fill="ln(eDNA)")
make_map_bathy(m.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Model 1",fill="ln(eDNA)",x="",y="")+
  ylim(orbbox[c(2,4)])+xlim(orbbox[c(1,3)])
ggsave(here('plots','Model 1 Oregon.png'),w=6,h=5,bg="white")
make_map_bathy(m.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Model 1",fill="ln(eDNA)",x="",y="")+
  ylim(wabbox[c(2,4)])+xlim(wabbox[c(1,3)])
ggsave(here('plots','Model 1 Washington.png'),w=6,h=5,bg="white")

make_map_bathy(m.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="",fill="ln(eDNA)",x="",y="")+
  ylim(wobbox[c(2,4)])+xlim(wobbox[c(1,3)])+
  coord_sf(datum=NA)
ggsave(here('plots','Model 1 WA and OR.png'),w=6,h=8,bg="white")

make_map_bathy(m.preds,est)+
    scale_fill_viridis(option="C")+
  labs(title="Model 1",fill="ln(eDNA)",x="",y="")+
  ylim(cabbox[c(2,4)])+xlim(cabbox[c(1,3)])
ggsave(here('plots','Model 1 California.png'),w=6,h=5,bg="white")

# quantiles?
m.preds %>% 
  arrange(est) %>% 
  mutate(quantest=ntile(est,100)) %>% 
  mutate(quantcut=cut(quantest,10)) %>% 
  mutate(top5=ifelse(quantest>=95,T,F)) %>% 
  make_map(top5)+
  scale_fill_manual(values=c('gray20','#BD3786FF'))+
  # scale_fill_viridis_c()+
    # scale_fill_manual(breaks=seq(0,1,by=0.1),values=viridis_pal(option="C")(11))+
  labs(title="Model 1",fill="Top 5% (eDNA)")

m.preds %>% 
  arrange(est) %>% 
  mutate(quantest=ntile(est,100)) %>% 
  mutate(quantcut=cut(quantest,10)) %>% 
  make_map(quantcut)+
  scale_fill_viridis(option='C',discrete=T)+
    # scale_fill_manual(breaks=seq(0,1,by=0.1),values=viridis_pal(option="C")(11))+
  labs(title="Model 1",fill="Quantile (eDNA)")

#presence/absence
m.pa.map <- make_presence_absence_map(m)[[2]]+labs(x="",y="")+
  ylim(wobbox[c(2,4)])+xlim(wobbox[c(1,3)])
ggsave(here('plots','Model 1 presence absence.png'),w=6,h=5,bg="white")
```

# Post-hoc Comparison

Compare our output to river input layer.

```{r}
m.preds.rivers <- m.preds %>% left_join(riverine_influence) %>% 
  ggplot(aes(river_input,est))+
  geom_point()+
  facet_grid(year~depth_cat,scales='free_x')+
  geom_smooth()
m.preds.rivers
```


# Test Mesh Complexity

One thing that seems to have an effect on our fits is the configuration of the INLA mesh, particularly its complexity, or number of knots. Meshes that are too complex can make pretty maps, but we run the risk of overfitting. As a general rule of thumb, we want to pick a less complex mesh, but one that still allows us to make good predictions. Let's see if we can use cross-validation likelihoods to assess which mesh to use.

```{r}
# we are using a combined INLA/sdmTMB workflow to make these meshes. We will vary the "cutoff", which determines the minimum triangle edge length, and in practice determines the number of vertices in the mesh
locs <- d_obs_filt %>%
  distinct(year,station,utm.lon.km,utm.lat.km) %>%
  st_as_sf(coords=c("utm.lon.km", "utm.lat.km"))

max.edge = diff(range(st_coordinates(locs)[,1]))/4
bound.outer = diff(range(st_coordinates(locs)[,1]))/3

domain <-fmesher::fm_nonconvex_hull(locs,
                              concave = -0.025,
                              convex = -0.025)
make_test_mesh <- function(test_cutoff){
  testmesh <- fmesher::fm_mesh_2d_inla(
    #loc=locs[,c("utm.lon","utm.lat")],
    loc.domain = domain, # coordinates
    boundary=domain,
    max.edge = c(max.edge,max.edge*2), # max triangle edge length; inner and outer meshes
    offset = c(max.edge, bound.outer),  # inner and outer border widths
    #max.n.strict=100,#,
    min.angle=15,
    cutoff = test_cutoff # minimum triangle edge length
    )
  make_mesh(d_obs_filt,c("utm.lon.km","utm.lat.km"),mesh=testmesh)
}

# let's make a series of meshes of increasing complexity, then fit our model and test its likelihood
cutoffs_to_try <- max.edge/seq(1.5,8,by=0.5)
meshes <- map(cutoffs_to_try,make_test_mesh)
# knots_to_try <- seq(50,175,by=25)
# meshes <- map(knots_to_try,~make_mesh(d_obs_filt,c("utm.lon.km", "utm.lat.km"),n_knots=.x))
map(meshes,plot)
```

Now try to fit the same model on each mesh

```{r}
calc_mesh_ll <- function(testmesh){
  fit <- try(sdmTMB_cv(
    Ct ~ 1+s(thetao_ln,k=3)+washed,
    data = d_obs_filt,
    mesh = testmesh,
    offset= "offsets_all",
    spatial="on",
    fold_ids=clust,
    family = stdcurve(),
    control = sdmTMBcontrol(stdcurve_df = d)
  ),
  silent=T)
  if(class(fit)=='try-error'){
    print(paste('Error.'))
    return(NA_real_)
  }
  # sanity checks
  s <- map(fit$models,function(x)sanity(x)$all_ok)
  # s <- sanity(fit$models[[2]])$all_ok
  if(any(s==FALSE)){
    print(paste('Error.'))
    return(NA_real_)
  } else{
    # sum_loglik = fit %>% 
    #   pluck('data') %>% 
    #   dplyr::filter(cv_fold==2) %>% 
    #   pluck('cv_loglik') %>% 
    #   sum()
  # print("CVLL:",m_cv$sum_loglik)
    return(fit$sum_loglik)
  }
}

tic("CV mesh analysis")
meshes.ll <- map_dbl(meshes,calc_mesh_ll)
toc()

```

Plot the relationship between mesh complexity and the 3-fold CV likelihood. Knots is one way to measure, but there's also total vertices (`mesh$mesh$n`)

```{r}
mesh.comparison <- tibble(cutoff=cutoffs_to_try,sumll=meshes.ll) %>% 
  ggplot(aes(cutoff,sumll))+
  geom_line()+
  geom_point()+
  # stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1,se=F)+
  geom_smooth(se=F)+
  labs(x="Minimum Interior Edge Length",y="Summed Log Likelihood")
mesh.comparison

mesh.comparison2 <- tibble(cutoff=cutoffs_to_try,sumll=meshes.ll) %>% 
  mutate(nverts=map_dbl(meshes,~.$mesh$n)) %>% 
  ggplot(aes(nverts,sumll))+
  geom_line()+
  geom_point()+
  geom_smooth(se=F)+
  # stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1,se=F)+
  scale_x_continuous(limits=c(100,400),breaks=seq(100,400,by=100))+
  labs(x="Number of Vertices",y="Summed Log Likelihood")
mesh.comparison2

meshes_table <- tibble(cutoff=cutoffs_to_try,sumll=meshes.ll) %>% 
  mutate(nverts=map_dbl(meshes,~.$mesh$n))

meshes.edges.nverts <- meshes_table%>% 
  ggplot(aes(cutoffs_to_try,nverts))+
  # geom_line()+
  geom_point()+
  labs(y="Number of Vertices",x="Minimum Interior Edge Length")

meshes.p <-plot_grid(mesh.comparison,mesh.comparison2,nrow=1)

meshes.edges.nverts
meshes.p

ggsave(here('plots','mesh_complexity_test.png'),meshes.p,h=4,w=7,bg='white')
```

Looks like one of the more complex meshes (15.6 minimum edge length, 317 vertices) provides the highest likelihood model.


# Scratch

```{r}
# try simpler meshes
meshestest <- map(seq(10,100,by=10),function(cuts) make_mesh(d_obs_filt,xy_cols=c('utm.lon.km','utm.lat.km'),type='cutoff',cutoff=cuts))

# let's make a series of meshes of increasing complexity, then fit our model and test its likelihood
meshes.ll.test <- map_dbl(meshestest,calc_mesh_ll)
```

```{r}
mesh.comparison <- tibble(cutoff=seq(10,100,by=10),sumll=meshes.ll.test) %>% 
  ggplot(aes(cutoff,sumll))+
  geom_line()+
  geom_point()+
  # stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1,se=F)+
  geom_smooth(se=F)+
  labs(x="Minimum Interior Edge Length",y="Summed Log Likelihood")
mesh.comparison

mesh.comparison2 <- tibble(cutoff=seq(10,100,by=10),sumll=meshes.ll.test) %>% 
  mutate(nverts=map_dbl(meshestest,~.$mesh$n)) %>% 
  ggplot(aes(nverts,sumll))+
  geom_line()+
  geom_point()+
  geom_smooth(se=F)+
  # stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1,se=F)+
  # scale_x_continuous(limits=c(100,400),breaks=seq(100,400,by=100))+
  labs(x="Number of Vertices",y="Summed Log Likelihood")
mesh.comparison2

meshes_table <- tibble(cutoffs_to_try=seq(10,100,by=10),sumll=meshes.ll.test) %>% 
  mutate(nverts=map_dbl(meshestest,~.$mesh$n))

meshes.edges.nverts <- meshes_table%>% 
  ggplot(aes(cutoffs_to_try,nverts))+
  # geom_line()+
  geom_point()+
  labs(y="Number of Vertices",x="Minimum Interior Edge Length")

meshes.p <-plot_grid(mesh.comparison,mesh.comparison2,nrow=1)

meshes.edges.nverts
meshes.p
```


